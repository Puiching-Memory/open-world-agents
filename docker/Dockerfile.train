# OWA Training Image - ML Training Environment with Flash Attention
# Multi-stage build for efficient flash-attention compilation

# Build arguments
ARG RUNTIME_IMAGE=owa/runtime:latest
ARG CONDA_INSTALL_PATH=/opt/conda

# =============================================================================
# Stage 1: Base ML Environment
# =============================================================================
FROM ${RUNTIME_IMAGE} AS ml-base

# Install core ML packages using uv with cache mounts
RUN --mount=type=cache,target=${HOME}/.cache/uv,sharing=locked \
    --mount=type=cache,target=${HOME}/.cache/pip,sharing=locked \
    . activate owa && vuv pip install \
    torch torchvision \
    numpy einops pillow \
    transformers datasets accelerate trl peft huggingface-hub \
    tqdm wandb

# =============================================================================
# Stage 2: Flash Attention Builder
# =============================================================================
FROM nvidia/cuda:12.6.3-devel-ubuntu24.04 AS flash-attn-builder

ARG CONDA_INSTALL_PATH

# Environment configuration
ENV DEBIAN_FRONTEND=noninteractive \
    USER=root UID=0 GID=0 HOME=/root \
    VUV_ALLOW_BASE=true PATH="${CONDA_INSTALL_PATH}/bin:${PATH}"

# Set shell for RUN commands
SHELL ["/bin/bash", "-c"]

# Copy conda environment from ml-base stage
COPY --from=ml-base ${CONDA_INSTALL_PATH} ${CONDA_INSTALL_PATH}

# Install build dependencies and compile flash-attn
RUN --mount=type=cache,target=${HOME}/.cache/uv,sharing=locked \
    --mount=type=cache,target=${HOME}/.cache/pip,sharing=locked \
    . activate owa && \
    vuv pip install ninja packaging && \
    vuv pip install flash-attn --no-build-isolation

# =============================================================================
# Stage 3: Final Training Image
# =============================================================================
FROM ml-base AS final

ARG CONDA_INSTALL_PATH

# Copy compiled flash-attn from builder stage. Following list can be acquired by "uninstalling" flash-attn, or by `pip show -f flash-attn`
COPY --from=flash-attn-builder ${CONDA_INSTALL_PATH}/envs/owa/lib/python3.11/site-packages/flash_attn ${CONDA_INSTALL_PATH}/envs/owa/lib/python3.11/site-packages/flash_attn
COPY --from=flash-attn-builder ${CONDA_INSTALL_PATH}/envs/owa/lib/python3.11/site-packages/flash_attn-2.8.0.post2.dist-info ${CONDA_INSTALL_PATH}/envs/owa/lib/python3.11/site-packages/flash_attn-2.8.0.post2.dist-info
COPY --from=flash-attn-builder ${CONDA_INSTALL_PATH}/envs/owa/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so ${CONDA_INSTALL_PATH}/envs/owa/lib/python3.11/site-packages/
COPY --from=flash-attn-builder ${CONDA_INSTALL_PATH}/envs/owa/lib/python3.11/site-packages/hopper ${CONDA_INSTALL_PATH}/envs/owa/lib/python3.11/site-packages/hopper

# Following command creates +10GB layer, so not using it for now
# COPY --from=flash-attn-builder ${CONDA_INSTALL_PATH}/envs/owa/lib/python3.11/site-packages ${CONDA_INSTALL_PATH}/envs/owa/lib/python3.11/site-packages

# Set working directory
WORKDIR /workspace

# Default command
CMD ["/bin/bash"]
